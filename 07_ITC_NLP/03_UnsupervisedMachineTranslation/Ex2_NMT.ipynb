{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ex2_NMT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "CKQdeaqDVUDq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# seq2seq Workshop Excercise 2: Transliteration\n",
        "\n",
        "In this excercise we will train a seq2seq model to transliterate Hebrew text into Latin characters, without any prior knowledge of Hebrew.\n",
        "\n",
        "## Part 1: Hebrew Unicode\n",
        "\n",
        "For our purposes it will be useful to know a bit about how text in Hebrew is encoded in Python strings.\n",
        "\n",
        "Recall that in Python a string is made up of **characters** than can be accessed with square brackets. The length of the string is the number of characters it contains:\n"
      ]
    },
    {
      "metadata": {
        "id": "SAC8qjCQl3OI",
        "colab_type": "code",
        "outputId": "75c8d395-3e86-4727-91a5-155bda735a2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"hello\"[1], \"hello\"[4], len(\"hello\"))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e o 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WK7rA8V6l5SE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In Python 3, a string is a sequence of **Unicode code points**, or unique numeric identifiers for each character. Python lets us see the Unicode code point for a character by using the built-in function *ord*:"
      ]
    },
    {
      "metadata": {
        "id": "_ybIlFDVnc1P",
        "colab_type": "code",
        "outputId": "90f1a311-8cd9-406c-eb3a-78a0a0949d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Unicode code points for characters in 'hello':\", *[ord(char) for char in \"hello\"])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unicode code points for characters in 'hello': 104 101 108 108 111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UB3rxxdjnowo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Questions**\n",
        "  1. What are the Unicode code points for each character in the word \"naivete\"? What about when it is written \"na茂vet茅\"?\n",
        "  2. Use the built-in Python function *hex* to get the hexidecimal (base-16) values for these code points. What are they?\n",
        "  3. Use the [Show Unicode Character](http://qaz.wtf/u/show.cgi) tool to look at the Unicode characters in each of these two words. Where can we see the code point values? What about the names of the unicode characters?\n",
        "  4. What is the difference between the words \"na茂vet茅\" and \"naivete\"? What is the length of each as a Python string?"
      ]
    },
    {
      "metadata": {
        "id": "cXXjYGmYjYo_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9d08ef68-6349-4981-b42d-1b6a15c1ed9d"
      },
      "cell_type": "code",
      "source": [
        "# 1\n",
        "print(\"Unicode code points for characters in 'naivete':\", *[ord(char) for char in \"naivete\"])\n",
        "print(\"Unicode code points for characters in 'na茂vet茅':\", *[ord(char) for char in \"na茂vet茅\"])\n",
        "# The encoding for i and 茂 or e and 茅 are completely different in the unicode mapping table\n",
        "\n",
        "# 2\n",
        "print(\"\\nHex values of Unicode characters in 'na茂vet茅':\", *[hex(ord(char)) for char in \"na茂vet茅\"])\n",
        "\n",
        "# 3\n",
        "# The Show Unicode Character displays the Unicode value of each character, followed by the hex value, the character itself and a plain English description\n",
        "\n",
        "# 4\n",
        "print(\"\\nAlthough they look identical, len('na茂vet茅')={} and len('naivete')={}.\".format(len('na茂vet茅'), len('naivete')))\n",
        "print(\"This is not magic, but one is using the combining character feature of Unicode where the other uses one character that renders identically\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unicode code points for characters in 'naivete': 110 97 105 118 101 116 101\n",
            "Unicode code points for characters in 'na茂vet茅': 110 97 239 118 101 116 233\n",
            "\n",
            "Hex values of Unicode characters in 'na茂vet茅': 0x6e 0x61 0xef 0x76 0x65 0x74 0xe9\n",
            "\n",
            "Although they look identical, len('na茂vet茅')=7 and len('naivete')=9.\n",
            "This is not magic, but one is using the combining character feature of Unicode where the other uses one character that renders identically\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WVMF2oPLpyS3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Hebrew words can be written either without vowels, or with vowel symbols called **nikkud**. Let's consider how these are represented in Python and in Unicode.\n",
        "\n",
        "**Questions:**\n",
        "  5. What are the first and last letters in the Python string for the Hebrew word 砖转? What are their hexidecimal Unicode codepoints?\n",
        "  6. How many characters does the Hebrew string 址旨职砖指旨转 have? Why this number?\n",
        "  7. What are the second and third characters of 执砖职专指值? What are their hexidecimal Unicode codepoints?\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "IOh85TGCovWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "39b5b812-5923-4876-c3ba-b39b664b0e75"
      },
      "cell_type": "code",
      "source": [
        "# 5\n",
        "print(\"First and last letters of 砖转 are respectively: {} and {}\".format('砖转'[0],'砖转'[-1]))\n",
        "print(\"This proves us that right-to-left is only a display feature but in memory the characters are in the correct order\")\n",
        "\n",
        "# 6\n",
        "print(\"\\nThe word '址旨职砖指旨转' has a length equal to {}. This is because each nikkud is a combining character to its letter.\".format(len('址旨职砖指旨转')))\n",
        "print(\"In the word '址旨职砖指旨转', we can count 6 nikkudim in addition to the 6 letters of the word giving a count of 12.\")\n",
        "\n",
        "# 7\n",
        "print(\"\\nThe second and third characters of '执砖职专指值' are respectively: {} and {}, with hex values: {} and {}.\".format('执砖职专指值'[1], '执砖职专指值'[2], hex(ord('执砖职专指值'[1])), hex(ord('执砖职专指值'[2]))))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First and last letters of 砖转 are respectively:  and 转\n",
            "This proves us that right-to-left is only a display feature but in memory the characters are in the correct order\n",
            "\n",
            "The word '址旨职砖指旨转' has a length equal to 12. This is because each nikkud is a combining character to its letter.\n",
            "In the word '址旨职砖指旨转', we can count 6 nikkudim in addition to the 6 letters of the word giving a count of 12.\n",
            "\n",
            "The second and third characters of '执砖职专指值' are respectively: 执 and 砖, with hex values: 0x5b4 and 0x5e9.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J4RQLvApaGT4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2: Data processing\n",
        "\n",
        "We'll be using the data in the attached file *nikkud_seq2seq_data.csv* to train and test our model. This contains Hebrew words without nikkud (vowels), the words with nikkud, and their transliterations (pronunciation written in Latin characters), scraped from articles on the [Hebrew-language Wiktionary](https://he.wiktionary.org/wiki/%D7%A2%D7%9E%D7%95%D7%93_%D7%A8%D7%90%D7%A9%D7%99).\n",
        "\n",
        "**Questions:**\n",
        "  8. Load the data into a Pandas DataFrame variable *df*. How many entries does df contain? Looking at some sample entries, do the transliterations look correct?\n",
        "  9. See if you can find where the transliterations were taken from in Wiktionary. (follow the link above and search for the given words.)"
      ]
    },
    {
      "metadata": {
        "id": "RJ4Wufbtre3I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e188907f-ed4f-46a6-eb46-cfc8996fd77d"
      },
      "cell_type": "code",
      "source": [
        "# 8\n",
        "df = pd.read_csv('nikkud_seq2seq_data.csv')\n",
        "df.head()\n",
        "# The df contains 15490 entries. The transliterations look pretty good.\n",
        "\n",
        "# 9\n",
        "# The transliterations are available for each word page, in the '转 拽拽' table, on the '' row\n",
        "# An example is eugenika in the following page: https://he.wiktionary.org/wiki/%D7%90%D7%90%D7%95%D7%92%D7%A0%D7%99%D7%A7%D7%94"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nikkud</th>\n",
              "      <th>transliteration</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>驻职旨专止住职转值执转</td>\n",
              "      <td>prostetit</td>\n",
              "      <td>驻专住转转</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>值旨侄执拽指</td>\n",
              "      <td>eugenika</td>\n",
              "      <td>拽</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>值止执止驻执</td>\n",
              "      <td>e'ozinofil</td>\n",
              "      <td>驻</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>指旨执职</td>\n",
              "      <td>auting</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>指</td>\n",
              "      <td>av</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            nikkud transliteration        word\n",
              "0   驻职旨专止住职转值执转       prostetit    驻专住转转\n",
              "1    值旨侄执拽指        eugenika    拽\n",
              "2  值止执止驻执      e'ozinofil  驻\n",
              "3      指旨执职          auting     \n",
              "4              指              av          "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "6cl1CB4ct0LK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our model will be simpler if we pad all words to be the same length, and add start- and end-of-word characters. \n",
        "\n",
        "**Questions:**\n",
        "  10. Define variables *nikkud_maxlen* and *translit_maxlen* as the length of the longest word in the *nikkud* and *transliteration* columns, respectively. What are these lengths?\n",
        "  11. Define the function *pad_word* as shown in the comments below, to add start- and end-of-word characters to a word and pad it to a given length."
      ]
    },
    {
      "metadata": {
        "id": "YDngV5AS0JaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "07560c67-3c24-4a33-8441-3fe29a8b50c1"
      },
      "cell_type": "code",
      "source": [
        "nikkud_maxlen = df.nikkud.apply(len).max()\n",
        "translit_maxlen = df.transliteration.apply(len).max()\n",
        "print(\"Longest word in the 'nikkud' and 'transliteration' columns are respectively: {} and {}\".format(nikkud_maxlen, translit_maxlen))\n",
        "\n",
        "def pad_word(word, pad_length):\n",
        "  #### add code here so the function adds ^ to the beginning of the word, spaces  after the word, and $ at the end\n",
        "  #### so that the output string is of length pad_length\n",
        "  #### example: pad_word(\"hello\", 12) should return the string \"^hello     $\" which is of length 12\n",
        "  padding = pad_length-len(word)-2 if pad_length-len(word)-2 > 0 else 0\n",
        "  return '^%s%s$' % (word, ' '*padding)\n",
        "\n",
        "print(\"\\nTesting pad_word('hello', 12) = {}\".format(pad_word('hello', 12)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longest word in the 'nikkud' and 'transliteration' columns are respectively: 31 and 25\n",
            "\n",
            "Testing pad_word('hello', 12) = ^hello     $\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AHU7IevT1Bp5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we define strings containing all characters used in our words, along with starting, padding, and ending tokens:"
      ]
    },
    {
      "metadata": {
        "id": "Y_qNYYAbK2ey",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nikkud_charset = '^$ ' + ''.join(sorted(set(''.join(df.nikkud))))\n",
        "translit_charset = '^$ ' + ''.join(sorted(set(''.join(df.transliteration))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q8MRAi9H1SsM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "  12. How many characters are used in words with nikkud? In transliterations?\n",
        "  13. Try printing out these character sets? Do you see anything strange in the output? Why?"
      ]
    },
    {
      "metadata": {
        "id": "mbxjzW3I2BQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "42f33f0c-05db-47b7-c1eb-bef17867204a"
      },
      "cell_type": "code",
      "source": [
        "# 12\n",
        "print(\"{} characters are used in the column 'nikkud' and {} in 'transliterations'\".format(len(nikkud_charset), len(translit_charset)))\n",
        "\n",
        "# 13\n",
        "print(\"\\nNikkud character set:\\n{}\".format(nikkud_charset))\n",
        "print(\"\\nTransliteration character set:\\n{}\".format(translit_charset))\n",
        "\n",
        "print(\"\\nThe tav '转' character had all the nikkudim merge with it, because these characters are special characters that attach to the preceding letter! \")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46 characters are used in the column 'nikkud' and 31 in 'transliterations'\n",
            "\n",
            "Nikkud character set:\n",
            "^$ \"'职直植殖执值侄址指止只旨住注祝驻抓爪拽专砖转\n",
            "\n",
            "Transliteration character set:\n",
            "^$ \"'abcdefghijklmnopqrstuvwxyz\n",
            "\n",
            "The tav '转' character had all the nikkudim merge with it, because these characters are special characters that attach to the preceding letter! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5H9VUMPJ1sqL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's define functions to produce sequence vectors from words with nikkud or transliterations:"
      ]
    },
    {
      "metadata": {
        "id": "klIXd7952tFj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nikkud2sequence(nikkud):\n",
        "  return [nikkud_charset.index(c) for c in pad_word(nikkud, nikkud_maxlen + 2)]\n",
        "def translit2sequence(translit):\n",
        "  return [translit_charset.index(c) for c in pad_word(translit, translit_maxlen + 2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "onX7bnt53LJv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "  14. What are the feature vectors for \"砖指止\" and \"shalom\"? What do the numbers in the vectors mean?\n",
        "  15. Add code to the comment below, to define functions *nikkud2onehot* and *translit2onehot*. These should take in strings (either a Hebrew word with nikkud, or a transliteration) and return a matrix where each character is one-hot encoded. Hint: Use *tf.keras.utils.to_categorical*, with attribute *num_classes = (number of characters in the character set)*.\n",
        "  16. If you implemented those functions correctly, nikkud2onehot('砖指止').shape should equal (33, 46) and translit2onehot('shalom').shape should equal (27, 31). What do these dimensions mean?"
      ]
    },
    {
      "metadata": {
        "id": "tM_aLkgn4OE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "00d6bfcc-b044-4282-a25c-4ca9d6f49c71"
      },
      "cell_type": "code",
      "source": [
        "# 14\n",
        "print(\"Vectors for '砖指止' and 'shalom' are respectively:\\n{} \\nand \\n{}\".format(nikkud2sequence('砖指止'), translit2sequence('shalom')))\n",
        "      \n",
        "# 15\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def nikkud2onehot(word):\n",
        "  return to_categorical(nikkud2sequence(word), num_classes=46)\n",
        "      \n",
        "def translit2onehot(word):\n",
        "  return to_categorical(translit2sequence(word), num_classes=31)\n",
        "  \n",
        "\n",
        "# 16\n",
        "print(\"\\nThe shapes of nikkud2onehot('砖指止') and translit2onehot('shalom') are {} and {}.\".format(nikkud2onehot('砖指止').shape, translit2onehot('shalom').shape))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectors for '砖指止' and 'shalom' are respectively:\n",
            "[0, 44, 13, 17, 31, 24, 14, 32, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1] \n",
            "and \n",
            "[0, 23, 12, 5, 16, 19, 17, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]\n",
            "\n",
            "The shapes of nikkud2onehot('砖指止') and translit2onehot('shalom') are (33, 46) and (27, 31).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wxq05Qv85osT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's combine the matrixes for all the words together into tensors:"
      ]
    },
    {
      "metadata": {
        "id": "lNnVXLE0LVQ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.array([nikkud2onehot(nikkud) for nikkud in df.nikkud])\n",
        "Y = np.array([translit2onehot(translit) for translit in df.transliteration])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jnvo2gdD5xYS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice that the first dimension of each tensor is the sample size (number of words):"
      ]
    },
    {
      "metadata": {
        "id": "y9iKrVCj5uu8",
        "colab_type": "code",
        "outputId": "33347ed9-0f18-48af-cefe-e07bfed6fdcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X.shape, Y.shape"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((15490, 33, 46), (15490, 27, 31))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "qMpe8ST-8BSJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the seq2seq model that we will train, we will try to predict the next character in the transliteration from the characters already generated and from the given nikkud. Since Y contains the encoding for the characters in the transliteration, we want to shift it by one to represent the next character that needs to be predicted.  This is simple with the numpy function *np.roll*. We save this in the tensor Z which will be predicted by the model given X (nikkud) and Y (transliteration):"
      ]
    },
    {
      "metadata": {
        "id": "QoPnnICX7_uE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Z = np.roll(Y, -1, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6myoxPO61FF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3: Seq2seq with LSTMs:"
      ]
    },
    {
      "metadata": {
        "id": "X0y3g2MB9T4a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll now build a seq2seq model with Keras to predict transliteration from nikkud. First let's build and train our model:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "d2DN--WILk47",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        },
        "outputId": "5c4c8044-b858-4d35-cdd3-8d28e2d58392"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "latent_dim = 256\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape = (None, len(nikkud_charset))) ## BONUS\n",
        "encoder = tf.keras.layers.LSTM(latent_dim, return_state = True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape = (None, len(translit_charset))) ## BONUS\n",
        "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences = True, return_state = True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(len(translit_charset), activation = 'softmax') ## BONUS\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')\n",
        "model.fit([X, Y], Z, batch_size = 256, epochs = 100, validation_split = 0.2)\n",
        "\n",
        "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = tf.keras.layers.Input(shape = (latent_dim,))\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape = (latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
        "                                    initial_state = decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = tf.keras.models.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12392 samples, validate on 3098 samples\n",
            "Epoch 1/100\n",
            "12392/12392 [==============================] - 10s 817us/step - loss: 1.1787 - val_loss: 1.0123\n",
            "Epoch 2/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.9413 - val_loss: 0.8583\n",
            "Epoch 3/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.8734 - val_loss: 0.7867\n",
            "Epoch 4/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.8034 - val_loss: 0.8217\n",
            "Epoch 5/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.7519 - val_loss: 0.7138\n",
            "Epoch 6/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.7140 - val_loss: 0.7248\n",
            "Epoch 7/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.6916 - val_loss: 0.7082\n",
            "Epoch 8/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.6770 - val_loss: 0.6924\n",
            "Epoch 9/100\n",
            "12392/12392 [==============================] - 7s 577us/step - loss: 0.6656 - val_loss: 0.7238\n",
            "Epoch 10/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.6584 - val_loss: 0.7021\n",
            "Epoch 11/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.6518 - val_loss: 0.6466\n",
            "Epoch 12/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.6450 - val_loss: 0.6384\n",
            "Epoch 13/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.6391 - val_loss: 0.6663\n",
            "Epoch 14/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.6358 - val_loss: 0.6457\n",
            "Epoch 15/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.6280 - val_loss: 0.6535\n",
            "Epoch 16/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.6243 - val_loss: 0.6274\n",
            "Epoch 17/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.6088 - val_loss: 0.6239\n",
            "Epoch 18/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.5943 - val_loss: 0.6291\n",
            "Epoch 19/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.5848 - val_loss: 0.5781\n",
            "Epoch 20/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.5709 - val_loss: 0.6861\n",
            "Epoch 21/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.5632 - val_loss: 0.5539\n",
            "Epoch 22/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.5469 - val_loss: 0.5675\n",
            "Epoch 23/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.5334 - val_loss: 0.5929\n",
            "Epoch 24/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.5200 - val_loss: 0.5548\n",
            "Epoch 25/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.5044 - val_loss: 0.5117\n",
            "Epoch 26/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.4913 - val_loss: 0.4790\n",
            "Epoch 27/100\n",
            "12392/12392 [==============================] - 7s 558us/step - loss: 0.4773 - val_loss: 0.4673\n",
            "Epoch 28/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.4636 - val_loss: 0.4545\n",
            "Epoch 29/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.4473 - val_loss: 0.4432\n",
            "Epoch 30/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.4411 - val_loss: 0.4248\n",
            "Epoch 31/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.4268 - val_loss: 0.4178\n",
            "Epoch 32/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.4040 - val_loss: 0.4254\n",
            "Epoch 33/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.3995 - val_loss: 0.5270\n",
            "Epoch 34/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.3869 - val_loss: 0.3948\n",
            "Epoch 35/100\n",
            "12392/12392 [==============================] - 7s 563us/step - loss: 0.3774 - val_loss: 0.4102\n",
            "Epoch 36/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.3606 - val_loss: 0.3457\n",
            "Epoch 37/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.3440 - val_loss: 0.3397\n",
            "Epoch 38/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.3328 - val_loss: 0.3321\n",
            "Epoch 39/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.3209 - val_loss: 0.3317\n",
            "Epoch 40/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.3130 - val_loss: 0.3177\n",
            "Epoch 41/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.3036 - val_loss: 0.3287\n",
            "Epoch 42/100\n",
            "12392/12392 [==============================] - 7s 560us/step - loss: 0.2916 - val_loss: 0.2955\n",
            "Epoch 43/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.2811 - val_loss: 0.2872\n",
            "Epoch 44/100\n",
            "12392/12392 [==============================] - 7s 562us/step - loss: 0.2687 - val_loss: 0.2776\n",
            "Epoch 45/100\n",
            "12392/12392 [==============================] - 7s 564us/step - loss: 0.2553 - val_loss: 0.2645\n",
            "Epoch 46/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.2471 - val_loss: 0.2398\n",
            "Epoch 47/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.2206 - val_loss: 0.4068\n",
            "Epoch 48/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.2112 - val_loss: 0.2350\n",
            "Epoch 49/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.2090 - val_loss: 0.2319\n",
            "Epoch 50/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.1975 - val_loss: 0.2046\n",
            "Epoch 51/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.1899 - val_loss: 0.1996\n",
            "Epoch 52/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1861 - val_loss: 0.1858\n",
            "Epoch 53/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.1788 - val_loss: 0.1917\n",
            "Epoch 54/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.1640 - val_loss: 0.1674\n",
            "Epoch 55/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.1603 - val_loss: 0.1862\n",
            "Epoch 56/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.1613 - val_loss: 0.1509\n",
            "Epoch 57/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.1294 - val_loss: 0.1546\n",
            "Epoch 58/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.1366 - val_loss: 0.1406\n",
            "Epoch 59/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.1433 - val_loss: 0.1436\n",
            "Epoch 60/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.1146 - val_loss: 0.4947\n",
            "Epoch 61/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.1358 - val_loss: 0.3364\n",
            "Epoch 62/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.1173 - val_loss: 0.1659\n",
            "Epoch 63/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.1237 - val_loss: 0.1220\n",
            "Epoch 64/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0971 - val_loss: 0.1389\n",
            "Epoch 65/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.1187 - val_loss: 0.1395\n",
            "Epoch 66/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.1095 - val_loss: 0.1402\n",
            "Epoch 67/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0990 - val_loss: 0.1170\n",
            "Epoch 68/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.0828 - val_loss: 0.1135\n",
            "Epoch 69/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.1026 - val_loss: 0.1211\n",
            "Epoch 70/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0997 - val_loss: 0.1136\n",
            "Epoch 71/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.0826 - val_loss: 0.2202\n",
            "Epoch 72/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.0863 - val_loss: 0.1088\n",
            "Epoch 73/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0933 - val_loss: 0.1144\n",
            "Epoch 74/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0861 - val_loss: 0.1377\n",
            "Epoch 75/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0644 - val_loss: 0.1103\n",
            "Epoch 76/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0968 - val_loss: 0.1265\n",
            "Epoch 77/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0612 - val_loss: 0.1067\n",
            "Epoch 78/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0769 - val_loss: 0.1232\n",
            "Epoch 79/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0748 - val_loss: 0.1076\n",
            "Epoch 80/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.0734 - val_loss: 0.1156\n",
            "Epoch 81/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0644 - val_loss: 0.3418\n",
            "Epoch 82/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0579 - val_loss: 0.1216\n",
            "Epoch 83/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0712 - val_loss: 0.1014\n",
            "Epoch 84/100\n",
            "12392/12392 [==============================] - 7s 561us/step - loss: 0.0484 - val_loss: 0.1056\n",
            "Epoch 85/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0690 - val_loss: 0.1137\n",
            "Epoch 86/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0668 - val_loss: 0.1072\n",
            "Epoch 87/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0435 - val_loss: 0.1079\n",
            "Epoch 88/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0687 - val_loss: 0.1012\n",
            "Epoch 89/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0417 - val_loss: 0.1149\n",
            "Epoch 90/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0684 - val_loss: 0.1074\n",
            "Epoch 91/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0636 - val_loss: 0.1536\n",
            "Epoch 92/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0370 - val_loss: 0.1137\n",
            "Epoch 93/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0557 - val_loss: 0.1044\n",
            "Epoch 94/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0557 - val_loss: 0.1004\n",
            "Epoch 95/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0342 - val_loss: 0.1029\n",
            "Epoch 96/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.0517 - val_loss: 0.1131\n",
            "Epoch 97/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0564 - val_loss: 0.1720\n",
            "Epoch 98/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.0323 - val_loss: 0.1248\n",
            "Epoch 99/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0475 - val_loss: 0.1054\n",
            "Epoch 100/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.0498 - val_loss: 0.1142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r9HjABKj9cu7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Based on this model, we can decode transliteration from nikkud one character at a time, at each step taking the most likely next character predicted by the model. The function *nikkud2translit* takes in a nikkud string and returns the predicted transliteration:"
      ]
    },
    {
      "metadata": {
        "id": "P-lpIGu5LsvJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_text, input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1, len(translit_charset))) ## BONUS\n",
        "    target_seq[0, 0, 0] = 1.\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "        char_probabilities = {\n",
        "            c: p for c, p in zip(translit_charset, output_tokens[0, -1, :]) ## BONUS\n",
        "        }\n",
        "        sampled_char = max(translit_charset, key = lambda c: char_probabilities[c]) ## BONUS\n",
        "        sampled_token_index = translit_charset.index(sampled_char) ## BONUS\n",
        "        decoded_sentence += sampled_char\n",
        "        if (sampled_char == '$' or\n",
        "           len(decoded_sentence) > translit_maxlen): ## BONUS\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1, len(translit_charset))) ## BONUS\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n",
        "def nikkud2translit(nikkud):\n",
        "  tensor = nikkud2onehot(nikkud)[None] ## BONUS\n",
        "  return decode_sequence(nikkud, tensor).replace('$', '').strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EE2GWL95BckG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "  18. Make a new dataframe *df2* containing 100 random samples from *df*. Add a new column *predicted_translit* to the dataframe *df2* with the model's predicted transliteration of the given nikkud. How often does this equal the actual transliteration? What kinds of errors do you see in the output?\n",
        "  17. Change the value of *epochs =* above to train the model on more epochs. How does this affect the loss? How about the observed results?\n",
        "\n",
        "**Bonus:** Modify the problem so that we are instead predicting Hebrew text with nikkud from a transliteration. You will have to switch X and Y, and change code where the comment ## BONUS is written above."
      ]
    },
    {
      "metadata": {
        "id": "Z9vPeAe6Nt7z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "outputId": "1edb4db6-87b2-4a3b-a254-6d5dc70ae456"
      },
      "cell_type": "code",
      "source": [
        "# 18\n",
        "df2 = df.sample(100)\n",
        "df2['predicted_translit'] = df2.nikkud.apply(nikkud2translit)\n",
        "print(\"The model's transliteration is equal to the actual transliteration {}/100 times\\n\".format((df2.transliteration == df2.predicted_translit).sum()))\n",
        "df2.head(20)\n",
        "\n",
        "# The model is pretty much never spot on. Many times though, the first letter of the prediction matches,\n",
        "# which seems to indicate the LSTM model was not trained enough in order to predict well the following letters..."
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model's transliteration is equal to the actual transliteration 0/100 times\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nikkud</th>\n",
              "      <th>transliteration</th>\n",
              "      <th>word</th>\n",
              "      <th>predicted_translit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8424</th>\n",
              "      <td>址职指</td>\n",
              "      <td>malta</td>\n",
              "      <td></td>\n",
              "      <td>ma'ara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10869</th>\n",
              "      <td>注植指执</td>\n",
              "      <td>'avadim</td>\n",
              "      <td>注</td>\n",
              "      <td>arakha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5659</th>\n",
              "      <td>址注止侄</td>\n",
              "      <td>ha'ogen</td>\n",
              "      <td>注</td>\n",
              "      <td>ma'ara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2994</th>\n",
              "      <td>址旨</td>\n",
              "      <td>gal</td>\n",
              "      <td></td>\n",
              "      <td>bara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1586</th>\n",
              "      <td>侄爪职指旨注止</td>\n",
              "      <td>etsba'on</td>\n",
              "      <td>爪注</td>\n",
              "      <td>arika</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3811</th>\n",
              "      <td>侄旨砖侄</td>\n",
              "      <td>deshe</td>\n",
              "      <td>砖</td>\n",
              "      <td>beret</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11727</th>\n",
              "      <td>驻旨止驻职旨拽止专职</td>\n",
              "      <td>popkorn</td>\n",
              "      <td>驻驻拽专</td>\n",
              "      <td>kharat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11962</th>\n",
              "      <td>驻侄</td>\n",
              "      <td>fen</td>\n",
              "      <td>驻</td>\n",
              "      <td>khara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>止拽旨职执职</td>\n",
              "      <td>okultizm</td>\n",
              "      <td>拽</td>\n",
              "      <td>arakha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11850</th>\n",
              "      <td>驻执旨执职</td>\n",
              "      <td>piling</td>\n",
              "      <td>驻</td>\n",
              "      <td>khilut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3564</th>\n",
              "      <td>执旨住职职旨专指驻职指</td>\n",
              "      <td>disgrafya</td>\n",
              "      <td>住专驻</td>\n",
              "      <td>ginit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4859</th>\n",
              "      <td>侄侄</td>\n",
              "      <td>khevel</td>\n",
              "      <td></td>\n",
              "      <td>kheret</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13805</th>\n",
              "      <td>专侄职专止住职驻侄旨拽职执指</td>\n",
              "      <td>retrospektiva</td>\n",
              "      <td>专专住驻拽</td>\n",
              "      <td>birut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>760</th>\n",
              "      <td>执侄专职执</td>\n",
              "      <td>inerti</td>\n",
              "      <td>专</td>\n",
              "      <td>inter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13352</th>\n",
              "      <td>拽职专执</td>\n",
              "      <td>kri</td>\n",
              "      <td>拽专</td>\n",
              "      <td>birut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6125</th>\n",
              "      <td>止专侄</td>\n",
              "      <td>yore</td>\n",
              "      <td>专</td>\n",
              "      <td>bara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8926</th>\n",
              "      <td>址注植专指止</td>\n",
              "      <td>markhon</td>\n",
              "      <td>注专</td>\n",
              "      <td>ma'ara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14030</th>\n",
              "      <td>砖指旨注址</td>\n",
              "      <td>shavua</td>\n",
              "      <td>砖注</td>\n",
              "      <td>kharash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3442</th>\n",
              "      <td>爪执止专止转</td>\n",
              "      <td>tsinorot</td>\n",
              "      <td>爪专转</td>\n",
              "      <td>khilut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12383</th>\n",
              "      <td>执旨住职止</td>\n",
              "      <td>bisdom</td>\n",
              "      <td>住</td>\n",
              "      <td>birut</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     nikkud transliteration         word predicted_translit\n",
              "8424                址职指           malta                      ma'ara\n",
              "10869              注植指执         'avadim        注             arakha\n",
              "5659                址注止侄         ha'ogen         注             ma'ara\n",
              "2994                   址旨             gal                          bara\n",
              "1586            侄爪职指旨注止        etsba'on       爪注              arika\n",
              "3811                侄旨砖侄           deshe          砖              beret\n",
              "11727         驻旨止驻职旨拽止专职         popkorn      驻驻拽专             kharat\n",
              "11962                   驻侄             fen           驻              khara\n",
              "410          止拽旨职执职        okultizm    拽             arakha\n",
              "11850            驻执旨执职          piling       驻             khilut\n",
              "3564       执旨住职职旨专指驻职指       disgrafya     住专驻              ginit\n",
              "4859                  侄侄          khevel                       kheret\n",
              "13805  专侄职专止住职驻侄旨拽职执指   retrospektiva  专专住驻拽              birut\n",
              "760              执侄专职执          inerti       专              inter\n",
              "13352                 拽职专执             kri          拽专              birut\n",
              "6125                 止专侄            yore         专               bara\n",
              "8926             址注植专指止         markhon       注专             ma'ara\n",
              "14030              砖指旨注址          shavua         砖注            kharash\n",
              "3442              爪执止专止转        tsinorot       爪专转             khilut\n",
              "12383             执旨住职止          bisdom        住              birut"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "tPUAFKNkQR6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "88ea836e-9cb9-4187-8dc2-78271f900557"
      },
      "cell_type": "code",
      "source": [
        "# 19\n",
        "df2 = df.sample(100)\n",
        "df2['predicted_translit'] = df2.nikkud.apply(nikkud2translit)\n",
        "print(\"The new model's transliteration is correct {}/100 times\".format((df2.transliteration == df2.predicted_translit).sum()))\n",
        "df2.head(20)\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The new model's transliteration is correct 90/100 times\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nikkud</th>\n",
              "      <th>transliteration</th>\n",
              "      <th>word</th>\n",
              "      <th>predicted_translit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12613</th>\n",
              "      <td>爪址侄旨专侄转</td>\n",
              "      <td>tzameret</td>\n",
              "      <td>爪专转</td>\n",
              "      <td>tsameret</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12898</th>\n",
              "      <td>拽止职执旨指止专执拽指</td>\n",
              "      <td>kombinatorika</td>\n",
              "      <td>拽专拽</td>\n",
              "      <td>komyofiya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2926</th>\n",
              "      <td>执旨指旨专止</td>\n",
              "      <td>gizaron</td>\n",
              "      <td>专</td>\n",
              "      <td>gizaron</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11761</th>\n",
              "      <td>驻址旨</td>\n",
              "      <td>pakh</td>\n",
              "      <td>驻</td>\n",
              "      <td>pakh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15030</th>\n",
              "      <td>转旨止专址砖职转执旨</td>\n",
              "      <td>torashti</td>\n",
              "      <td>转专砖转</td>\n",
              "      <td>torashti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>址职止住职执</td>\n",
              "      <td>agnosti</td>\n",
              "      <td>住</td>\n",
              "      <td>agnosti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9411</th>\n",
              "      <td>职砖只砖侄旨</td>\n",
              "      <td>meshushe</td>\n",
              "      <td>砖砖</td>\n",
              "      <td>meshushe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8961</th>\n",
              "      <td>植指执</td>\n",
              "      <td>haga'im</td>\n",
              "      <td></td>\n",
              "      <td>haga'im</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13626</th>\n",
              "      <td>驻指旨砖旨</td>\n",
              "      <td>pashut</td>\n",
              "      <td>驻砖</td>\n",
              "      <td>pashut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5600</th>\n",
              "      <td>址砖职止址</td>\n",
              "      <td>khashmonay</td>\n",
              "      <td>砖</td>\n",
              "      <td>khashmonay</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3508</th>\n",
              "      <td>侄旨侄专职侄旨职</td>\n",
              "      <td>detergnat</td>\n",
              "      <td>专</td>\n",
              "      <td>detergnat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9485</th>\n",
              "      <td>执砖职值砖</td>\n",
              "      <td>mishmesh</td>\n",
              "      <td>砖砖</td>\n",
              "      <td>mishmesh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2962</th>\n",
              "      <td>址砖职址执旨转</td>\n",
              "      <td>khashmalit</td>\n",
              "      <td>砖转</td>\n",
              "      <td>khashmalit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>777</th>\n",
              "      <td>值拽址专职止执</td>\n",
              "      <td>ekaryot</td>\n",
              "      <td>拽专</td>\n",
              "      <td>ekaryot</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8524</th>\n",
              "      <td>执旨旨</td>\n",
              "      <td>minuy</td>\n",
              "      <td></td>\n",
              "      <td>minuy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13590</th>\n",
              "      <td>专止砖</td>\n",
              "      <td>rosh</td>\n",
              "      <td>专砖</td>\n",
              "      <td>rosh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>956</th>\n",
              "      <td>值址转</td>\n",
              "      <td>elat</td>\n",
              "      <td>转</td>\n",
              "      <td>elat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1644</th>\n",
              "      <td>址执旨</td>\n",
              "      <td>yami</td>\n",
              "      <td></td>\n",
              "      <td>yami</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8754</th>\n",
              "      <td>执住职驻指旨专</td>\n",
              "      <td>mispar</td>\n",
              "      <td>住驻专</td>\n",
              "      <td>mispar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8929</th>\n",
              "      <td>侄拽止止执转</td>\n",
              "      <td>ekologit</td>\n",
              "      <td>拽转</td>\n",
              "      <td>ekologit</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     nikkud transliteration          word predicted_translit\n",
              "12613              爪址侄旨专侄转        tzameret          爪专转           tsameret\n",
              "12898  拽止职执旨指止专执拽指   kombinatorika  拽专拽          komyofiya\n",
              "2926             执旨指旨专止         gizaron         专            gizaron\n",
              "11761                  驻址旨            pakh            驻               pakh\n",
              "15030         转旨止专址砖职转执旨        torashti        转专砖转           torashti\n",
              "160            址职止住职执         agnosti       住            agnosti\n",
              "9411             职砖只砖侄旨        meshushe         砖砖           meshushe\n",
              "8961               植指执         haga'im                     haga'im\n",
              "13626              驻指旨砖旨          pashut          驻砖             pashut\n",
              "5600           址砖职止址      khashmonay       砖         khashmonay\n",
              "3508          侄旨侄专职侄旨职       detergnat        专          detergnat\n",
              "9485              执砖职值砖        mishmesh          砖砖           mishmesh\n",
              "2962           址砖职址执旨转      khashmalit        砖转         khashmalit\n",
              "777            值拽址专职止执         ekaryot       拽专            ekaryot\n",
              "8524                执旨旨           minuy                        minuy\n",
              "13590                 专止砖            rosh           专砖               rosh\n",
              "956                   值址转            elat           转               elat\n",
              "1644                 址执旨            yami                          yami\n",
              "8754               执住职驻指旨专          mispar          住驻专             mispar\n",
              "8929           侄拽止止执转        ekologit      拽转           ekologit"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "metadata": {
        "id": "kGQSCRx6VuCR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "73db9a5f-8508-423f-90cd-eb189a51918d"
      },
      "cell_type": "code",
      "source": [
        "# Extra words not in the training set:\n",
        "print(\"Transliteration of '{}' returned {}\".format('执住职址职转执旨', nikkud2translit('执住职址职转执旨')))\n",
        "print(\"Transliteration of '{}' returned {}\".format('转职执指旨转执', nikkud2translit('转职执指旨转执')))\n",
        "print(\"Transliteration of '{}' returned {}\".format('址职址爪值旨址', nikkud2translit('址职址爪值旨址')))\n",
        "\n",
        "# Althought the results against the training test are pretty impressive,\n",
        "# we can see here that the generalization is not as impressive, altghough not too far"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transliteration of '执住职址职转执旨' returned nismakhmit\n",
            "Transliteration of '转职执指旨转执' returned tila'it\n",
            "Transliteration of '址职址爪值旨址' returned lamatstana\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pFhgEUJmN5AW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        },
        "outputId": "0f8ac078-8095-4342-b08b-7db70dca918c"
      },
      "cell_type": "code",
      "source": [
        "# BONUS QUESTION\n",
        "Z = np.roll(X, -1, axis = 1)\n",
        "latent_dim = 256\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape = (None, len(translit_charset)))\n",
        "encoder = tf.keras.layers.LSTM(latent_dim, return_state = True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape = (None, len(nikkud_charset)))\n",
        "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences = True, return_state = True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(len(nikkud_charset), activation = 'softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')\n",
        "model.fit([Y, X], Z, batch_size = 256, epochs = 100, validation_split = 0.2)\n",
        "\n",
        "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = tf.keras.layers.Input(shape = (latent_dim,))\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape = (latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
        "                                    initial_state = decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = tf.keras.models.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "def decode_sequence_to_heb(input_text, input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1, len(nikkud_charset)))\n",
        "    target_seq[0, 0, 0] = 1.\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "        char_probabilities = {\n",
        "            c: p for c, p in zip(nikkud_charset, output_tokens[0, -1, :])\n",
        "        }\n",
        "        sampled_char = max(nikkud_charset, key = lambda c: char_probabilities[c])\n",
        "        sampled_token_index = nikkud_charset.index(sampled_char)\n",
        "        decoded_sentence += sampled_char\n",
        "        if (sampled_char == '$' or\n",
        "           len(decoded_sentence) > nikkud_maxlen):\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1, len(nikkud_charset)))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n",
        "def translit2nikkud(translit):\n",
        "  tensor = translit2onehot(translit)[None]\n",
        "  return decode_sequence_to_heb(translit, tensor).replace('$', '').strip()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12392 samples, validate on 3098 samples\n",
            "Epoch 1/100\n",
            "12392/12392 [==============================] - 10s 808us/step - loss: 1.3600 - val_loss: 1.1594\n",
            "Epoch 2/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 1.0812 - val_loss: 1.0299\n",
            "Epoch 3/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.9679 - val_loss: 0.9021\n",
            "Epoch 4/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.8501 - val_loss: 0.7959\n",
            "Epoch 5/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.7629 - val_loss: 0.7769\n",
            "Epoch 6/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.7052 - val_loss: 0.7177\n",
            "Epoch 7/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.6723 - val_loss: 0.6699\n",
            "Epoch 8/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.6465 - val_loss: 0.6425\n",
            "Epoch 9/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.6277 - val_loss: 0.6444\n",
            "Epoch 10/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.6150 - val_loss: 0.6091\n",
            "Epoch 11/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.6000 - val_loss: 0.6194\n",
            "Epoch 12/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.5800 - val_loss: 0.6057\n",
            "Epoch 13/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.5641 - val_loss: 0.5696\n",
            "Epoch 14/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.5469 - val_loss: 0.5601\n",
            "Epoch 15/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.5299 - val_loss: 0.5642\n",
            "Epoch 16/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.5156 - val_loss: 0.5291\n",
            "Epoch 17/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.4995 - val_loss: 0.5176\n",
            "Epoch 18/100\n",
            "12392/12392 [==============================] - 7s 576us/step - loss: 0.4815 - val_loss: 0.5364\n",
            "Epoch 19/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.4666 - val_loss: 0.4852\n",
            "Epoch 20/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.4455 - val_loss: 0.4689\n",
            "Epoch 21/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.4254 - val_loss: 0.4527\n",
            "Epoch 22/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.4031 - val_loss: 0.4361\n",
            "Epoch 23/100\n",
            "12392/12392 [==============================] - 7s 576us/step - loss: 0.3833 - val_loss: 0.4201\n",
            "Epoch 24/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.3674 - val_loss: 0.3955\n",
            "Epoch 25/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.3482 - val_loss: 0.3929\n",
            "Epoch 26/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.3324 - val_loss: 0.3740\n",
            "Epoch 27/100\n",
            "12392/12392 [==============================] - 7s 576us/step - loss: 0.3169 - val_loss: 0.3623\n",
            "Epoch 28/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.3021 - val_loss: 0.3999\n",
            "Epoch 29/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.2873 - val_loss: 0.3496\n",
            "Epoch 30/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.2772 - val_loss: 0.3171\n",
            "Epoch 31/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.2616 - val_loss: 0.2964\n",
            "Epoch 32/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.2530 - val_loss: 0.2794\n",
            "Epoch 33/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.2403 - val_loss: 0.2715\n",
            "Epoch 34/100\n",
            "12392/12392 [==============================] - 7s 578us/step - loss: 0.2322 - val_loss: 0.2627\n",
            "Epoch 35/100\n",
            "12392/12392 [==============================] - 7s 576us/step - loss: 0.2208 - val_loss: 0.2654\n",
            "Epoch 36/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.2129 - val_loss: 0.2642\n",
            "Epoch 37/100\n",
            "12392/12392 [==============================] - 7s 577us/step - loss: 0.2043 - val_loss: 0.2520\n",
            "Epoch 38/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1961 - val_loss: 0.2414\n",
            "Epoch 39/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.1871 - val_loss: 0.2400\n",
            "Epoch 40/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1821 - val_loss: 0.2274\n",
            "Epoch 41/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1728 - val_loss: 0.2646\n",
            "Epoch 42/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.1670 - val_loss: 0.2198\n",
            "Epoch 43/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.1619 - val_loss: 0.2077\n",
            "Epoch 44/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1532 - val_loss: 0.2134\n",
            "Epoch 45/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.1493 - val_loss: 0.2147\n",
            "Epoch 46/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.1442 - val_loss: 0.1932\n",
            "Epoch 47/100\n",
            "12392/12392 [==============================] - 7s 566us/step - loss: 0.1373 - val_loss: 0.2005\n",
            "Epoch 48/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1395 - val_loss: 0.1899\n",
            "Epoch 49/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.1309 - val_loss: 0.1715\n",
            "Epoch 50/100\n",
            "12392/12392 [==============================] - 7s 567us/step - loss: 0.1188 - val_loss: 0.1867\n",
            "Epoch 51/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.1244 - val_loss: 0.1648\n",
            "Epoch 52/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1268 - val_loss: 0.1666\n",
            "Epoch 53/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.1062 - val_loss: 0.2378\n",
            "Epoch 54/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.1085 - val_loss: 0.1509\n",
            "Epoch 55/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.1108 - val_loss: 0.1624\n",
            "Epoch 56/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.1053 - val_loss: 0.2036\n",
            "Epoch 57/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.1011 - val_loss: 0.1570\n",
            "Epoch 58/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.0993 - val_loss: 0.1599\n",
            "Epoch 59/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0983 - val_loss: 0.1958\n",
            "Epoch 60/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.0936 - val_loss: 0.1667\n",
            "Epoch 61/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0928 - val_loss: 0.1963\n",
            "Epoch 62/100\n",
            "12392/12392 [==============================] - 7s 569us/step - loss: 0.0836 - val_loss: 0.1568\n",
            "Epoch 63/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0812 - val_loss: 0.1585\n",
            "Epoch 64/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0827 - val_loss: 0.1951\n",
            "Epoch 65/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.0785 - val_loss: 0.1542\n",
            "Epoch 66/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0746 - val_loss: 0.1569\n",
            "Epoch 67/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0798 - val_loss: 0.1727\n",
            "Epoch 68/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.0735 - val_loss: 0.1521\n",
            "Epoch 69/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0685 - val_loss: 0.1758\n",
            "Epoch 70/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.0686 - val_loss: 0.1539\n",
            "Epoch 71/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.0633 - val_loss: 0.1507\n",
            "Epoch 72/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0629 - val_loss: 0.1955\n",
            "Epoch 73/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0682 - val_loss: 0.1512\n",
            "Epoch 74/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0594 - val_loss: 0.1572\n",
            "Epoch 75/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0568 - val_loss: 0.1619\n",
            "Epoch 76/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0529 - val_loss: 0.3876\n",
            "Epoch 77/100\n",
            "12392/12392 [==============================] - 7s 570us/step - loss: 0.0645 - val_loss: 0.2136\n",
            "Epoch 78/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.0556 - val_loss: 0.1727\n",
            "Epoch 79/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0521 - val_loss: 0.1601\n",
            "Epoch 80/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0545 - val_loss: 0.1611\n",
            "Epoch 81/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0522 - val_loss: 0.1576\n",
            "Epoch 82/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0479 - val_loss: 0.1647\n",
            "Epoch 83/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0473 - val_loss: 0.1681\n",
            "Epoch 84/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.0492 - val_loss: 0.1522\n",
            "Epoch 85/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0458 - val_loss: 0.1701\n",
            "Epoch 86/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0317 - val_loss: 0.1639\n",
            "Epoch 87/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.0534 - val_loss: 0.1570\n",
            "Epoch 88/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.0419 - val_loss: 0.1672\n",
            "Epoch 89/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0369 - val_loss: 0.1833\n",
            "Epoch 90/100\n",
            "12392/12392 [==============================] - 7s 568us/step - loss: 0.0360 - val_loss: 0.1744\n",
            "Epoch 91/100\n",
            "12392/12392 [==============================] - 7s 565us/step - loss: 0.0430 - val_loss: 0.1588\n",
            "Epoch 92/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.0497 - val_loss: 0.1674\n",
            "Epoch 93/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0396 - val_loss: 0.1631\n",
            "Epoch 94/100\n",
            "12392/12392 [==============================] - 7s 575us/step - loss: 0.0333 - val_loss: 0.1702\n",
            "Epoch 95/100\n",
            "12392/12392 [==============================] - 7s 573us/step - loss: 0.0354 - val_loss: 0.1695\n",
            "Epoch 96/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.0301 - val_loss: 0.1884\n",
            "Epoch 97/100\n",
            "12392/12392 [==============================] - 7s 574us/step - loss: 0.0292 - val_loss: 0.1673\n",
            "Epoch 98/100\n",
            "12392/12392 [==============================] - 7s 571us/step - loss: 0.0359 - val_loss: 0.1707\n",
            "Epoch 99/100\n",
            "12392/12392 [==============================] - 7s 572us/step - loss: 0.0339 - val_loss: 0.1733\n",
            "Epoch 100/100\n",
            "12392/12392 [==============================] - 7s 578us/step - loss: 0.0393 - val_loss: 0.1703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IY8hHi8STyZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "1af91f58-652c-47b9-cade-68e4c5b25f1c"
      },
      "cell_type": "code",
      "source": [
        "# BONUS - TESTING NEW MODEL\n",
        "df2 = df.sample(100)\n",
        "df2['predicted_nikkud'] = df2.transliteration.apply(translit2nikkud)\n",
        "print(\"The transliteration to nikkud model was correct {}/100 times\".format((df2.nikkud == df2.predicted_nikkud).sum()))\n",
        "df2.head(20)\n",
        "\n",
        "# Interestingly, most errors don't fall far away from the expected word.\n",
        "# For example, tsaar was predicted with an Aleph instead of an Ayin and a kamats instead of a Patah.\n",
        "# In this example and many other, the model was able to predict the correct pronunciation.\n",
        "\n",
        "# As these predictions are generated and not based on a dictionary, we could argue \n",
        "# that a prediction that preserves the pronunciation is valid which would boost our accuracy up a lot!"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The transliteration to nikkud model was correct 71/100 times\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nikkud</th>\n",
              "      <th>transliteration</th>\n",
              "      <th>word</th>\n",
              "      <th>predicted_nikkud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12653</th>\n",
              "      <td>爪址注址专</td>\n",
              "      <td>tsaar</td>\n",
              "      <td>爪注专</td>\n",
              "      <td>爪址指专</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4075</th>\n",
              "      <td>址驻指旨转旨址</td>\n",
              "      <td>hapatu'akh</td>\n",
              "      <td>驻转</td>\n",
              "      <td>址驻指旨转旨址</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8466</th>\n",
              "      <td>址职值专</td>\n",
              "      <td>mamzer</td>\n",
              "      <td>专</td>\n",
              "      <td>址职值专</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1548</th>\n",
              "      <td>侄驻职注侄</td>\n",
              "      <td>ef'e</td>\n",
              "      <td>驻注</td>\n",
              "      <td>侄驻职注侄</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13432</th>\n",
              "      <td>拽指砖侄</td>\n",
              "      <td>kashe</td>\n",
              "      <td>拽砖</td>\n",
              "      <td>拽指砖值</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11459</th>\n",
              "      <td>注值拽侄</td>\n",
              "      <td>ekel</td>\n",
              "      <td>注拽</td>\n",
              "      <td>侄侄</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12799</th>\n",
              "      <td>植执专</td>\n",
              "      <td>khazir</td>\n",
              "      <td>专</td>\n",
              "      <td>植执专</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5810</th>\n",
              "      <td>执职旨</td>\n",
              "      <td>tiltul</td>\n",
              "      <td></td>\n",
              "      <td>执职旨</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4159</th>\n",
              "      <td>指址职</td>\n",
              "      <td>halakh</td>\n",
              "      <td></td>\n",
              "      <td>指址职</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>指侄止驻址旨转职指</td>\n",
              "      <td>alelopatya</td>\n",
              "      <td>驻转</td>\n",
              "      <td>指侄止驻址旨转职指</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9478</th>\n",
              "      <td>执砖职址专</td>\n",
              "      <td>mishmar</td>\n",
              "      <td>砖专</td>\n",
              "      <td>执砖职址专</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9832</th>\n",
              "      <td>址址</td>\n",
              "      <td>nakhal</td>\n",
              "      <td></td>\n",
              "      <td>址址</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>指止址</td>\n",
              "      <td>amoday</td>\n",
              "      <td></td>\n",
              "      <td>指止址</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9649</th>\n",
              "      <td>执职指旨</td>\n",
              "      <td>nivdal</td>\n",
              "      <td></td>\n",
              "      <td>执职指旨</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10768</th>\n",
              "      <td>住执驻旨旨</td>\n",
              "      <td>sipun</td>\n",
              "      <td>住驻</td>\n",
              "      <td>住执驻旨旨</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7850</th>\n",
              "      <td>址职值专指</td>\n",
              "      <td>mazmera</td>\n",
              "      <td>专</td>\n",
              "      <td>址职值专指</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5582</th>\n",
              "      <td>指砖旨</td>\n",
              "      <td>chashud</td>\n",
              "      <td>砖</td>\n",
              "      <td>指砖旨</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9456</th>\n",
              "      <td>执砖执旨职止</td>\n",
              "      <td>mishikhmo</td>\n",
              "      <td>砖</td>\n",
              "      <td>执砖执旨职止</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12631</th>\n",
              "      <td>爪执旨止专</td>\n",
              "      <td>tsinor</td>\n",
              "      <td>爪专</td>\n",
              "      <td>爪执旨止专</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7237</th>\n",
              "      <td>止</td>\n",
              "      <td>tov</td>\n",
              "      <td></td>\n",
              "      <td>止</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                nikkud transliteration      word predicted_nikkud\n",
              "12653            爪址注址专           tsaar       爪注专            爪址指专\n",
              "4075        址驻指旨转旨址      hapatu'akh     驻转       址驻指旨转旨址\n",
              "8466           址职值专          mamzer      专          址职值专\n",
              "1548           侄驻职注侄            ef'e      驻注          侄驻职注侄\n",
              "13432           拽指砖侄           kashe       拽砖           拽指砖值\n",
              "11459            注值拽侄            ekel       注拽            侄侄\n",
              "12799           植执专          khazir      专           植执专\n",
              "5810          执职旨          tiltul              执职旨\n",
              "4159            指址职          halakh                  指址职\n",
              "906    指侄止驻址旨转职指      alelopatya  驻转  指侄止驻址旨转职指\n",
              "9478          执砖职址专         mishmar      砖专         执砖职址专\n",
              "9832             址址          nakhal                   址址\n",
              "990          指止址          amoday            指止址\n",
              "9649          执职指旨          nivdal               执职指旨\n",
              "10768          住执驻旨旨           sipun      住驻          住执驻旨旨\n",
              "7850         址职值专指         mazmera     专        址职值专指\n",
              "5582           指砖旨         chashud      砖          指砖旨\n",
              "9456       执砖执旨职止       mishikhmo     砖      执砖执旨职止\n",
              "12631          爪执旨止专          tsinor      爪专          爪执旨止专\n",
              "7237              止             tov                    止"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "metadata": {
        "id": "wRI7VUw4XP0t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c35a8096-817b-4966-a456-17f064a0a36c"
      },
      "cell_type": "code",
      "source": [
        "# Extra words not in the training set:\n",
        "print(\"Transliteration of '{}' returned {}\".format('trumpeldor', translit2nikkud('trumpeldor')))\n",
        "print(\"Transliteration of '{}' returned {}\".format('jeremy', translit2nikkud('jeremy')))\n",
        "print(\"Transliteration of '{}' returned {}\".format(\"raanana\", translit2nikkud(\"raanana\")))\n",
        "print(\"Transliteration of '{}' returned {}\".format(\"titkhadesh\", translit2nikkud(\"titkhadesh\")))\n",
        "\n",
        "# The performance on these words is interesting, but not as impressive\n",
        "# We can conclude that our model is very likely overfitting "
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transliteration of 'trumpeldor' returned 职专旨职侄旨职侄专职\n",
            "Transliteration of 'jeremy' returned 侄'专侄侄\n",
            "Transliteration of 'raanana' returned 植专指指止\n",
            "Transliteration of 'titkhadesh' returned 执职址职职\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}